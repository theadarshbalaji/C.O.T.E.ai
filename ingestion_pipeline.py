import os
import json
import re
from typing import List
from topic_mapper import group_elements_by_topic
from unstructured.partition.pdf import partition_pdf
from unstructured.chunking.title import chunk_by_title
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_chroma import Chroma
from langchain_core.messages import HumanMessage
from dotenv import load_dotenv
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)
import concurrent.futures
import threading

# Global semaphore to limit TOTAL concurrent API calls across all files
# Tier 1 has 2000 RPM but 1M TPM. Keeping this low prevents hitting the TPM limit with large chunks.
api_semaphore = threading.BoundedSemaphore(3)
# Note: Google's 429 error is often wrapped in an InternalServerError or similar in LangChain,
# but we can retry on general exceptions if they look like rate limits.

load_dotenv(override=True)

# --- CONFIGURATION ---
# Using local embeddings to avoid 429 rate limits during bulk upload

LOCAL_EMBEDDINGS = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
CHROMA_PATH = "./chroma_db"

# GEMINI_MODEL = "gemini-2.5-flash"  # Use for high-quality showcase
GEMINI_MODEL = "gemini-2.0-flash"  # Use for cost-effective testing

llm = ChatGoogleGenerativeAI(model=GEMINI_MODEL, temperature=0)

# --- CORE FUNCTIONS (Replicated from your notebook) ---

def is_valid_pdf(file_path: str) -> bool:
    try:
        with open(file_path, "rb") as f:
            header = f.read(5)
        return header == b"%PDF-"
    except Exception:
        return False


def partitioning_documents(file_path: str):
    """Safely extract elements from PDF with fallback strategies."""
    print(f"üìÑ Partitioning: {file_path}")

    if not is_valid_pdf(file_path):
        print(f"‚ùå Skipping invalid PDF: {file_path}")
        return []

    try:
        # Primary (best quality)
        return partition_pdf(
            filename=file_path,
            strategy="hi_res",
            infer_table_structure=True,
            extract_image_block_types=["Image"],
            extract_image_block_to_payload=True,
        )

    except Exception as e:
        print(f"‚ö†Ô∏è hi_res failed, falling back: {e}")

        try:
            # Fallback (text-only, very stable)
            return partition_pdf(
                filename=file_path,
                strategy="fast",
            )
        except Exception as e:
            print(f"‚ùå Failed to process PDF entirely: {e}")
            return []


def create_chunks_by_title(elements):
    """Uses your specific chunking strategy from the notebook."""
    return chunk_by_title(
        elements,
        max_characters=3000,
        new_after_n_chars=2400,
        combine_text_under_n_chars=500
    )

def separate_content_types(chunk):
    """Helper to extract text, tables, and images from Unstructured chunks."""
    content_data = {'text': chunk.text, 'tables': [], 'images': [], 'types': ['text']}
    if hasattr(chunk, 'metadata') and hasattr(chunk.metadata, 'orig_elements'):
        for element in chunk.metadata.orig_elements:
            element_type = type(element).__name__
            if element_type == 'Table':
                content_data['types'].append('table')
                content_data['tables'].append(getattr(element.metadata, 'text_as_html', element.text))
            elif element_type == 'Image' and hasattr(element.metadata, 'image_base64'):
                img_b64 = element.metadata.image_base64
                # --- FILTERING: Skip small icons/logos (< 10KB base64) to save Gemini quota ---
                if len(img_b64) > 10000:
                    content_data['types'].append('image')
                    content_data['images'].append(img_b64)
                else:
                    print("üîç Skipping small image/icon to save API quota.")
    content_data['types'] = list(set(content_data['types']))
    return content_data

@retry(
    stop=stop_after_attempt(10),
    wait=wait_exponential(multiplier=1, min=4, max=60),
    retry=retry_if_exception_type(Exception),
    before_sleep=lambda retry_state: print(f"‚ö†Ô∏è API Limit hit. Retrying in {retry_state.next_action.sleep} seconds...")
)
def create_batch_ai_summaries(batch_contents: List[dict]) -> List[str]:
    """Processes a batch of content blocks in a single Gemini call."""
    with api_semaphore: # Limit total concurrent calls
        if not batch_contents:
            return []

        prompt_text = (
            "You are an expert at analyzing mixed-content chunks from technical documents for a RAG system.\n"
            "Below are several content blocks. For each block, provide a concise summary that captures "
            "the key facts, concepts, and data. Respond with a JSON array of strings, where each string "
            "is the summary for the corresponding block.\n\n"
        )
        
        message_content = [{"type": "text", "text": prompt_text}]
        
        for i, content in enumerate(batch_contents):
            block_desc = f"--- BLOCK {i+1} ---\nTEXT:\n{content['text']}\n"
            if content['tables']:
                block_desc += f"TABLES:\n{chr(10).join(content['tables'])}\n"
            
            message_content.append({"type": "text", "text": block_desc})
            for img_b64 in content['images']:
                message_content.append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img_b64}"}})

        try:
            # Request JSON output
            response = llm.invoke([HumanMessage(content=message_content)])
            content_out = response.content.strip()
            
            # Strip markdown code blocks if present
            if content_out.startswith("```json"):
                content_out = content_out[7:-3].strip()
            elif content_out.startswith("```"):
                content_out = content_out[3:-3].strip()
                
            summaries = json.loads(content_out)
            if isinstance(summaries, list) and len(summaries) == len(batch_contents):
                return [str(s) for s in summaries]
            else:
                print(f"‚ö†Ô∏è Unexpected JSON format from LLM: {content_out}")
                return [c['text'] for c in batch_contents]
                
        except Exception as e:
            print(f"‚ùå Gemini summary failed: {e}")
            return [c['text'] for c in batch_contents]

def is_already_ingested(filename: str, session_id: str) -> bool:
    """Checks ChromaDB to see if this file has already been processed for this session."""
    try:
        db = Chroma(
            persist_directory=CHROMA_PATH,
            embedding_function=LOCAL_EMBEDDINGS,
            collection_name="hackathon_collection"
        )
        results = db.get(where={"$and": [{"source": filename}, {"session_id": session_id}]})
        return len(results['ids']) > 0
    except Exception:
        return False

def process_files_to_docs(directory_path: str) -> List[Document]:
    """Iterates through all PDFs in the session directory with batching, locking, and checkpointing."""
    all_docs = []
    session_id = os.path.basename(directory_path)
    
    files = [f for f in os.listdir(directory_path) if f.lower().endswith(".pdf")]
    total_files = len(files)
    
    print(f"üöÄ Starting ingestion for {total_files} files in session {session_id}")

    for idx, filename in enumerate(files):
        print(f"\n--- üìÑ Processing File {idx+1}/{total_files}: {filename} ---")
        
        # --- CHECKPOINTING: Skip if already in DB ---
        if is_already_ingested(filename, session_id):
            print(f"‚è≠Ô∏è Skipping {filename}: Already fully ingested in this session.")
            continue
            
        file_path = os.path.join(directory_path, filename)

        # 1. Partition
        elements = partitioning_documents(file_path)
        if not elements:
            print(f"‚ö†Ô∏è Skipping {filename}: No elements extracted.")
            continue
        print(f"‚úÖ Partitioning complete: {len(elements)} elements found.")
        
        # 2. Map Elements to Topics (Hierarchical Grouping)
        topics = group_elements_by_topic(elements)
        print(f"‚úÖ Topic mapping complete: {len(topics)} major topics identified.")

        topic_docs = []
        for topic in topics:
            topic_title = topic["title"]
            topic_elements = topic["elements"]

            # 3. Chunk elements within this topic
            chunks = create_chunks_by_title(topic_elements)
            
            # 4. Prepare contents and identify batch candidates
            chunk_data_list = []
            multimodal_indices = []
            
            for i, chunk in enumerate(chunks):
                content = separate_content_types(chunk)
                content['parent_topic'] = topic_title # Attach parent topic info
                chunk_data_list.append(content)
                if len(content['types']) > 1:
                    multimodal_indices.append(len(chunk_data_list) - 1)
            
            # 5. Process Multimodal Chunks in Parallel Batches for this Topic
            batch_size = 5
            if multimodal_indices:
                print(f"ü§ñ Processing {len(multimodal_indices)} multimodal chunks in topic: {topic_title}...")
                
                batches = []
                for i in range(0, len(multimodal_indices), batch_size):
                    batch_idxs = multimodal_indices[i : i + batch_size]
                    batches.append((batch_idxs, [chunk_data_list[idx] for idx in batch_idxs]))

                def process_batch(batch_data):
                    idxs, contents = batch_data
                    summaries = create_batch_ai_summaries(contents)
                    return idxs, summaries

                with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                    results = list(executor.map(process_batch, batches))

                    for batch_idxs, summaries in results:
                        for idx, summary in zip(batch_idxs, summaries):
                            chunk_data_list[idx]['ai_summary'] = summary

            # 6. Convert to LangChain Documents for this Topic
            for content in chunk_data_list:
                raw_text = content['text']
                ai_summary = content.get('ai_summary', '')
                
                if ai_summary:
                    indexed_content = f"TOPIC: {topic_title}\nSUMMARY: {ai_summary}\n\nORIGINAL TEXT: {raw_text}"
                else:
                    indexed_content = f"TOPIC: {topic_title}\n\n{raw_text}"

                doc = Document(
                    page_content=indexed_content,
                    metadata={
                        "session_id": session_id,
                        "source": filename,
                        "parent_topic": topic_title,
                        "original_content": json.dumps({
                            "raw_text": raw_text,
                            "tables_html": content['tables'],
                            "images_base64": content['images']
                        })
                    }
                )
                all_docs.append(doc)
            
        # 6. Inter-file cooldown (Removed for Tier 1)
        pass
            
    return all_docs

def create_vector_store(documents: List[Document]):
    """Stores documents in a persistent local ChromaDB."""
    print(f"Storing {len(documents)} chunks in ChromaDB...")
    return Chroma.from_documents(
        documents=documents,
        embedding=LOCAL_EMBEDDINGS,
        persist_directory=CHROMA_PATH,
        collection_name="hackathon_collection"
    )

# --- MAIN INGESTION ENTRY POINT ---

def ingest_directory(directory_path: str):
    """Function called by your FastAPI backend."""
    # 1. Process all files in the directory into chunks
    processed_docs = process_files_to_docs(directory_path)
    
    # 2. Store them in the vector database
    if processed_docs:
        create_vector_store(processed_docs)
        print(f"Successfully ingested session: {os.path.basename(directory_path)}")
    else:
        print("No valid documents found for ingestion.")

if __name__ == "__main__":
    # Standard test logic for standalone execution
    TEST_DIR = "./docs"
    if os.path.exists(TEST_DIR):
        print(f"üöÄ Starting standalone test ingestion for: {TEST_DIR}")
        ingest_directory(TEST_DIR)
    else:
        print(f"‚ö†Ô∏è Test directory {TEST_DIR} not found. Please create it and add PDFs to test.")